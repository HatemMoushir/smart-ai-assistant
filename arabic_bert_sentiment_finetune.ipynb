{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/smart-ai-assistant/blob/main/arabic_bert_sentiment_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2DHktnpNt2a"
      },
      "source": [
        "# ğŸ”¥ Fine-Tuning Arabic BERT on ArSentD-LEV Dataset\n",
        "Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ¯Ø±ÙŠØ¨ BERT Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© Hugging Face."
      ],
      "id": "b2DHktnpNt2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sff2yjKBNt2e"
      },
      "outputs": [],
      "source": [
        "# âœ… ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "!pip install -q transformers datasets evaluate scikit-learn"
      ],
      "id": "Sff2yjKBNt2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHXS7HDkNt2g"
      },
      "outputs": [],
      "source": [
        "# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('arsentd_lev')\n",
        "dataset = dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
        "train_ds = dataset['train']\n",
        "test_ds = dataset['test']"
      ],
      "id": "JHXS7HDkNt2g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWypdBVkNt2g"
      },
      "outputs": [],
      "source": [
        "# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø­ÙˆÙ„\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "model_name = 'asafaya/bert-base-arabic'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ],
      "id": "rWypdBVkNt2g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXXyXQJYNt2h"
      },
      "outputs": [],
      "source": [
        "# âœ… ØªÙˆÙƒÙ†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example['text'], padding='max_length', truncation=True)\n",
        "train_ds = train_ds.map(tokenize_function, batched=True)\n",
        "test_ds = test_ds.map(tokenize_function, batched=True)"
      ],
      "id": "OXXyXQJYNt2h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jFuXheKNt2h"
      },
      "outputs": [],
      "source": [
        "# âœ… Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=test_ds,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "id": "2jFuXheKNt2h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LndcFE6Nt2i"
      },
      "outputs": [],
      "source": [
        "# âœ… Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "trainer.train()"
      ],
      "id": "3LndcFE6Nt2i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1o2L_phtNt2j"
      },
      "outputs": [],
      "source": [
        "# âœ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨\n",
        "model.save_pretrained(\"./arabic-sentiment-bert\")\n",
        "tokenizer.save_pretrained(\"./arabic-sentiment-bert\")"
      ],
      "id": "1o2L_phtNt2j"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}