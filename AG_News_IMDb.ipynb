{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9WkWrTjSf7cIFSBu5HJUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/smart-ai-assistant/blob/main/AG_News_IMDb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAMQmjKpNryh"
      },
      "outputs": [],
      "source": [
        "# âœ… Notebooks: AG News & IMDb training with DistilBERT\n",
        "\n",
        "# --------- Part 1: AG News Training ---------\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load dataset\n",
        "ag_dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "# Load tokenizer and tokenize data\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\")\n",
        "\n",
        "ag_encoded = ag_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Load model for 4-class classification\n",
        "ag_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
        "\n",
        "# Evaluation metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Training arguments\n",
        "ag_args = TrainingArguments(\n",
        "    output_dir=\"ag_news_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "ag_trainer = Trainer(\n",
        "    model=ag_model,\n",
        "    args=ag_args,\n",
        "    train_dataset=ag_encoded[\"train\"].select(range(10000)),\n",
        "    eval_dataset=ag_encoded[\"test\"].select(range(1000)),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train AG News model\n",
        "ag_trainer.train()\n",
        "\n",
        "# --------- Part 2: IMDb Training (Manual Load) ---------\n",
        "\n",
        "# Download IMDb dataset\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def load_imdb_data(path):\n",
        "    data = {'text': [], 'label': []}\n",
        "    for label_type in ['pos', 'neg']:\n",
        "        folder = os.path.join(path, label_type)\n",
        "        for file in os.listdir(folder)[:3000]:  # Load 3000 per class for speed\n",
        "            with open(os.path.join(folder, file), 'r', encoding=\"utf-8\") as f:\n",
        "                data['text'].append(f.read())\n",
        "                data['label'].append(1 if label_type == 'pos' else 0)\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Load and convert to Dataset\n",
        "train_df = load_imdb_data(\"/content/aclImdb/train\")\n",
        "test_df = load_imdb_data(\"/content/aclImdb/test\")\n",
        "\n",
        "imdb_train = Dataset.from_pandas(train_df)\n",
        "imdb_test = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenize IMDb\n",
        "imdb_encoded = imdb_train.map(tokenize, batched=True)\n",
        "imdb_eval = imdb_test.map(tokenize, batched=True)\n",
        "\n",
        "# Load model for binary classification\n",
        "imdb_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
        "\n",
        "# Training arguments\n",
        "imdb_args = TrainingArguments(\n",
        "    output_dir=\"imdb_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "imdb_trainer = Trainer(\n",
        "    model=imdb_model,\n",
        "    args=imdb_args,\n",
        "    train_dataset=imdb_encoded,\n",
        "    eval_dataset=imdb_eval.select(range(1000)),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train IMDb model\n",
        "imdb_trainer.train()"
      ]
    }
  ]
}